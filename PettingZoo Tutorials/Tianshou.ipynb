{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tianshou Tutorial\n",
    "## Tianshou: Basic API Usage\n",
    "demonstrates a game between two random policy agents in the rock-paper-scissors environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 13:35:05.941191: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tianshou.data import Collector\n",
    "from tianshou.env import DummyVectorEnv, PettingZooEnv\n",
    "from tianshou.policy import MultiAgentPolicyManager, RandomPolicy\n",
    "\n",
    "from pettingzoo.classic import rps_v2\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # step 1: load the PettingZoo environment\n",
    "    env = rps_v2.env(render_mode=\"human\")\n",
    "\n",
    "    # step 2: wrap the environment for Tianshou interfacing\n",
    "    env = PettingZooEnv(env)\n",
    "\n",
    "    # step 3: define policies for each agent\n",
    "    policies = MultiAgentPolicyManager([RandomPolicy(), RandomPolicy()], env)\n",
    "\n",
    "    # step 4: convert the env to vector format\n",
    "    env = DummyVectorEnv([lambda: env])\n",
    "\n",
    "    # step 5: construct the Collector, which interfaces the policies with the vectorised environment\n",
    "    collector = Collector(policies, env)\n",
    "\n",
    "    # step 6: execute the environment with the agents playing for 1 episode, and render a frame every 0.1 seconds\n",
    "    result = collector.collect(n_episode=1, render=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tianshou: Training Agents\n",
    "use Tianshou to train a Deep Q-Network (DQN) agent to play vs a random policy agent in the Tic-Tac-Toe environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:01, 863.78it/s, env_step=1000, len=7, n/ep=5, n/st=50, player_2/loss=0.272, rew=-0.20]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 0.400000 ± 0.916515, best_reward: 0.400000 ± 0.916515 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #2: 1001it [00:01, 989.95it/s, env_step=2000, len=7, n/ep=8, n/st=50, player_2/loss=0.266, rew=-0.38]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 0.500000 ± 0.806226, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #3: 1001it [00:01, 992.77it/s, env_step=3000, len=6, n/ep=6, n/st=50, player_2/loss=0.260, rew=0.33]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -0.400000 ± 0.916515, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 1001it [00:01, 988.36it/s, env_step=4000, len=6, n/ep=7, n/st=50, player_2/loss=0.256, rew=0.43]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 0.300000 ± 0.900000, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 1001it [00:01, 867.87it/s, env_step=5000, len=7, n/ep=6, n/st=50, player_2/loss=0.251, rew=-0.67]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 0.200000 ± 0.979796, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #6: 1001it [00:01, 981.98it/s, env_step=6000, len=6, n/ep=4, n/st=50, player_2/loss=0.222, rew=-0.50]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: -0.200000 ± 0.979796, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #7: 1001it [00:01, 991.46it/s, env_step=7000, len=6, n/ep=9, n/st=50, player_2/loss=0.241, rew=0.22]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 0.000000 ± 1.000000, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #8: 1001it [00:01, 985.12it/s, env_step=8000, len=7, n/ep=8, n/st=50, player_2/loss=0.254, rew=0.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 0.000000 ± 1.000000, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 1001it [00:01, 659.99it/s, env_step=9000, len=7, n/ep=6, n/st=50, player_2/loss=0.247, rew=0.33]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 0.100000 ± 0.943398, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 1001it [00:01, 876.82it/s, env_step=10000, len=6, n/ep=7, n/st=50, player_2/loss=0.232, rew=-0.14]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: -0.200000 ± 0.979796, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 1001it [00:01, 711.62it/s, env_step=11000, len=6, n/ep=9, n/st=50, player_2/loss=0.252, rew=0.11]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: 0.400000 ± 0.916515, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 1001it [00:01, 780.30it/s, env_step=12000, len=6, n/ep=7, n/st=50, player_2/loss=0.246, rew=0.43]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: 0.300000 ± 0.900000, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 1001it [00:01, 935.91it/s, env_step=13000, len=7, n/ep=4, n/st=50, player_2/loss=0.235, rew=-0.25]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: 0.400000 ± 0.916515, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #14: 1001it [00:01, 888.91it/s, env_step=14000, len=7, n/ep=4, n/st=50, player_2/loss=0.252, rew=0.50]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: -0.100000 ± 0.943398, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 1001it [00:01, 816.05it/s, env_step=15000, len=6, n/ep=7, n/st=50, player_2/loss=0.239, rew=0.43]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: 0.400000 ± 0.916515, best_reward: 0.500000 ± 0.806226 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 1001it [00:01, 823.32it/s, env_step=16000, len=6, n/ep=8, n/st=50, player_2/loss=0.240, rew=0.50]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: 0.600000 ± 0.800000, best_reward: 0.600000 ± 0.800000 in #16\n",
      "\n",
      "==========Result==========\n",
      "{'duration': '19.09s', 'train_time/model': '11.04s', 'test_step': 1207, 'test_episode': 170, 'test_time': '0.55s', 'test_speed': '2198.96 step/s', 'best_reward': 0.6, 'best_result': '0.60 ± 0.80', 'train_step': 16000, 'train_episode': 2259, 'train_time/collector': '7.50s', 'train_speed': '863.03 step/s'}\n",
      "\n",
      "(the trained policy can be accessed via policy.policies[agents[1]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    env = _get_env()\n",
    "    observation_space = (\n",
    "        env.observation_space[\"observation\"]\n",
    "        if isinstance(env.observation_space, gymnasium.spaces.Dict)\n",
    "        else env.observation_space\n",
    "    )\n",
    "    if agent_learn is None:\n",
    "        # model\n",
    "        net = Net(\n",
    "            state_shape=observation_space.shape or observation_space.n,\n",
    "            action_shape=env.action_space.shape or env.action_space.n,\n",
    "            hidden_sizes=[128, 128, 128, 128],\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "        agent_learn = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor=0.9,\n",
    "            estimation_step=3,\n",
    "            target_update_freq=320\n",
    "        )\n",
    "    if agent_opponent is None:\n",
    "        agent_opponent = RandomPolicy()\n",
    "    \n",
    "    agents = [agent_opponent, agent_learn]\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, optim, env.agents\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This functions is needed to provide callables for DummyVectorEnv\"\"\"\n",
    "    return PettingZooEnv(tictactoe_v3.env())\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # step 1: environment setup\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(10)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(10)])\n",
    "\n",
    "    # seed\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # step 2: agent setup\n",
    "    policy, optim, agents = _get_agents()\n",
    "\n",
    "    # step 3: collector setup\n",
    "    train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(20_000, len(train_envs)),\n",
    "        exploration_noise=True\n",
    "    )\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "    # policy.set_eps(1)\n",
    "    train_collector.collect(n_step=64 * 10)  # batch size * training_num\n",
    "\n",
    "# step 4: callback functions setup\n",
    "def save_best_fn(policy):\n",
    "    model_save_path = os.path.join(\"log\", \"ttt\", \"dqn\", \"policy.pth\")\n",
    "    os.makedirs(os.path.join(\"log\", \"ttt\", \"dqn\"), exist_ok=True)\n",
    "    torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "\n",
    "def stop_fn(mean_rewards):\n",
    "    return mean_rewards >= 0.6\n",
    "\n",
    "def train_fn(epoch, env_step):\n",
    "    policy.policies[agents[1]].set_eps(0.1)\n",
    "\n",
    "def test_fn(epoch, env_step):\n",
    "    policy.policies[agents[1]].set_eps(0.05)\n",
    "\n",
    "def reward_metric(rews):\n",
    "    return rews[:, 1]\n",
    "\n",
    "# step 5: run the trainer\n",
    "result = offpolicy_trainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=50,\n",
    "    step_per_epoch=1000,\n",
    "    step_per_collect=50,\n",
    "    episode_per_test=10,\n",
    "    batch_size=64,\n",
    "    train_fn=train_fn,\n",
    "    test_fn=test_fn,\n",
    "    stop_fn=stop_fn,\n",
    "    save_best_fn=save_best_fn,\n",
    "    update_per_step=0.1,\n",
    "    test_in_train=False,\n",
    "    reward_metric=reward_metric\n",
    ")\n",
    "\n",
    "# return result, policy.policies[agents[1]]\n",
    "print(f\"\\n==========Result==========\\n{result}\")\n",
    "print(\"\\n(the trained policy can be accessed via policy.policies[agents[1]])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tianshou: CLI and Logging\n",
    "extends the code from Training Agents to add CLI (using argparse) and logging (using Tianshou's Logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def get_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--seed\", type=int, default=1626)\n",
    "    parser.add_argument(\"--eps-test\", type=float, default=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
