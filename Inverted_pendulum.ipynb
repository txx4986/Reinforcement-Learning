{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Pendulum\n",
    "**Objective**: To balance the pole (inverted pendulum) on top of the cart <br>\n",
    "\n",
    "**Actions**: The agent takes a 1D vector for actions. The action space is a continuous `(action)` in `[-3, 3]`, where action represents the numerical force applied to the cart (with magnitude representing the amount of force and sign representing the direction) <br>\n",
    "\n",
    "**Approach**: We use PyTorch to code REINFORCE from scratch to train a Neural Network policy to master inverted pendulum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network\n",
    "We start by building a policy that the agent will learn using REINFORCE. A policy is a mapping from the current environment observation to a probability distribution of the actions to be taken. The policy used in the tutorial is parameterised by a neural network. It consists of 2 linear layers that are shared between both the predicted mean and standard deviation. Further, the single individual linear layers are used to estimate the mean and the standard deviation. `nn.Tanh` is used as a non-linearity between the hidden layers. The following function estimates a mean and standard deviation of a normal distribution from which an action is sampled. Hence it is expected for the policy to learn appropriate weights to output means and standard deviation based on the current observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Network(nn.Module):\n",
    "    \"\"\"Parameterised Policy Network\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initialises a neural network that estimates the mean and standard deviation of a normal distribution from which an \n",
    "        action is sampled from.\n",
    "        \n",
    "        Args:\n",
    "            obs_space_dims: dimension of the observation space\n",
    "            action_space_dims: dimension of the action space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_space1 = 16\n",
    "        hidden_space2 = 32\n",
    "\n",
    "        # shared network\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(obs_space_dims, hidden_space1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_space1, hidden_space2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # policy mean specific linear layer\n",
    "        self.policy_mean_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_space_dims)\n",
    "        )\n",
    "\n",
    "        # policy std speific linear layer\n",
    "        self.policy_stddev_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_space_dims)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Conditioned on the observation, returns the mean and standard deviation of a normal distribution from which\n",
    "        an action is sampled from.\n",
    "        \n",
    "        Args:\n",
    "            x: observation from the environment\n",
    "        \n",
    "        Returns:\n",
    "            action_means: predicted mean of the normal distribution\n",
    "            action_stddevs: predicted standard deviation of the normal distribution\n",
    "        \"\"\"\n",
    "        shared_features = self.shared_net(x.float())\n",
    "\n",
    "        action_means = self.policy_mean_net(shared_features)\n",
    "        action_stddevs = torch.log(\n",
    "            1 + torch.exp(self.policy_stddev_net(shared_features))\n",
    "        )\n",
    "\n",
    "        return action_means, action_stddevs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an agent\n",
    "REINFORCE: Reward Increment Non-negative Factor times Offset Reinforcement times Characteristic Eligibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \"\"\"REINFORCE algorithm\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initialises an agent that learns a policy via REINFORCE algorithm.\n",
    "        \n",
    "        Args:\n",
    "            obs_space_dims: dimension of the observation space\n",
    "            action_space_dims: dimension of the action space\n",
    "        \"\"\"\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 1e-4  # learning rate for policy optimisation\n",
    "        self.gamma = 0.99  # discount factor\n",
    "        self.eps = 1e-6  # small number for mathematical stability\n",
    "\n",
    "        self.probs = []  # stores probability values of the sampled action\n",
    "        self.rewards = []  # stores the corresponding rewards\n",
    "\n",
    "        self.net = Policy_Network(obs_space_dims, action_space_dims)\n",
    "        self.optimiser = torch.optim.AdamW(self.net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def sample_action(self, state: np.ndarray) -> float:\n",
    "        \"\"\"Returns an action, conditioned on the policy and observation.\n",
    "        \n",
    "        Args:\n",
    "            state: observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            action: action to be performed\n",
    "        \"\"\"\n",
    "        state = torch.tensor(np.array([state]))\n",
    "        action_means, action_stddevs = self.net(state)\n",
    "\n",
    "        # create a normal distribution from the predicted mean and standard deviation and sample an action\n",
    "        distrib = Normal(action_means[0] + self.eps, action_stddevs[0] + self.eps)\n",
    "        action = distrib.sample()\n",
    "        prob = distrib.log_prob(action)\n",
    "\n",
    "        action = action.numpy()\n",
    "\n",
    "        self.probs.append(prob)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Updates the policy network's weights\"\"\"\n",
    "        running_g = 0\n",
    "        gs = []\n",
    "\n",
    "        # discounted return (backwards) - [::-1] will return an array in reverse\n",
    "        for R in self.rewards[::-1]:\n",
    "            running_g = R + self.gamma * running_g\n",
    "            gs.insert(0, running_g)\n",
    "        \n",
    "        deltas = torch.tensor(gs)\n",
    "\n",
    "        loss = 0\n",
    "        # minimise -1 * prob * reward obtained\n",
    "        for log_prob, delta in zip(self.probs, deltas):\n",
    "            loss += log_prob.mean() * delta * (-1)\n",
    "\n",
    "        # update the policy network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # empty/zero out all episode-centric/related variables\n",
    "        self.probs = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "    for seed in random seeds\n",
    "        reinitialise agent \n",
    "        for episode in range of max numbers of episodes\n",
    "            until episode is done\n",
    "                sample action based on current observation\n",
    "                take action and receive reward and next observation\n",
    "                store action taken, its probability, and the observed reward\n",
    "            update the policy\n",
    "\n",
    "Note: Deep RL is fairly brittle concerning random seed in a lot of common use cases. Hence it is important to test out various seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and wrap the environment\n",
    "env = gym.make(\"InvertedPendulum-v4\")\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, 50)  # records episode-reward\n",
    "\n",
    "total_num_episodes = int(5e3)  # total number of episodes\n",
    "# observation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
