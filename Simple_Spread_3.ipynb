{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Spread\n",
    "**Objectives**: The environment has N agents and N landmarks. Agents must learn to cover all the landmarks while avoiding collisions. All agents are globally rewarded based on how far the closest agents is to each landmark (sum of the minimum distances). The agents are penalised if they collide with other agents (-1 for each collision).\n",
    "\n",
    "**Actions**:\n",
    " - Agent observations: `[self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, communication]`\n",
    " - Agent action space: `[no_action, move_left, move_right, move_down, move_up]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 20:38:44.570099: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', type=int, default=1626)\n",
    "    parser.add_argument('--eps-test', type=float, default=0.05)\n",
    "    parser.add_argument('--eps-train', type=float, default=0.1)\n",
    "    parser.add_argument('--buffer-size', type=int, default=2000)\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--gamma', type=float, default=0.9, help='a smaller gamma favors earlier win')\n",
    "    parser.add_argument('--n-step', type=int, default=3)\n",
    "    parser.add_argument('--target-update-freq', type=int, default=320)\n",
    "    parser.add_argument('--epoch', type=int, default=50)\n",
    "    parser.add_argument('--step-per-epoch', type=int, default=1000)\n",
    "    parser.add_argument('--step-per-collect', type=int, default=10)\n",
    "    parser.add_argument('--update-per-step', type=float, default=0.1)\n",
    "    parser.add_argument('--batch-size', type=int, default=64)\n",
    "    parser.add_argument('--hidden-sizes', type=int, nargs='*', default=[128, 128, 128, 128])\n",
    "    parser.add_argument('--training-num', type=int, default=64)\n",
    "    parser.add_argument('--test-num', type=int, default=10)\n",
    "    parser.add_argument('--logdir', type=str, default='log')\n",
    "    parser.add_argument('--render', type=float, default=0.1)\n",
    "    parser.add_argument('--win-rate', type=float, default=0)\n",
    "    parser.add_argument('--watch', default=False, action='store_true', help='no training, watch the play of pre-trained models')\n",
    "    parser.add_argument('--agent-id', type=int, default=2)\n",
    "    parser.add_argument('--resume-path-0', type=str, default='', help='the path of agent pth file for resuming from a pre-trained agent 0')\n",
    "    parser.add_argument('--resume-path-1', type=str, default='', help='the path of agent pth file for resuming from a pre-trained agent 1')\n",
    "    parser.add_argument('--resume-path-2', type=str, default='', help='the path of agent pth file for resuming from a pre-trained agent 2')\n",
    "    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return parser\n",
    "\n",
    "def get_args() -> argparse.Namespace:\n",
    "    parser = get_parser()\n",
    "    return parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "\n",
    "def get_env(render_mode=None):\n",
    "    \"\"\"This functions is needed to provide callables for DummyVectorEnv\"\"\"\n",
    "    return PettingZooEnv(simple_spread_v3.env(N=num_agents, max_cycles=25, local_ratio=0.5, continuous_actions=False,\n",
    "                                              render_mode=\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agents(\n",
    "        args: argparse.Namespace = get_args(),\n",
    "        agent_0: Optional[BasePolicy] = None,\n",
    "        agent_1: Optional[BasePolicy] = None,\n",
    "        agent_2: Optional[BasePolicy] = None,\n",
    "        optim: Optional[torch.optim.Optimizer] = None\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    env = get_env()\n",
    "    observation_space = env.observation_space['observation'] if isinstance(env.observation_space, gym.spaces.Dict) else env.observation_space\n",
    "    args.state_shape = observation_space.shape or observation_space.n\n",
    "    args.action_shape = env.action_space.shape or env.action_space.n\n",
    "    if agent_0 is None:\n",
    "        # model\n",
    "        net = Net(\n",
    "            args.state_shape,\n",
    "            args.action_shape,\n",
    "            hidden_sizes=args.hidden_sizes,\n",
    "            device=args.device\n",
    "        ).to(args.device)\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "        agent_0 = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor=args.gamma,\n",
    "            action_space=env.action_space,\n",
    "            estimation_step=args.n_step,\n",
    "            target_update_freq=args.target_update_freq\n",
    "        )\n",
    "        if args.resume_path_0:\n",
    "            agent_0.load_state_dict(torch.load(args.resume_path_0))\n",
    "\n",
    "    if agent_1 is None:\n",
    "        # model\n",
    "        net = Net(\n",
    "            args.state_shape,\n",
    "            args.action_shape,\n",
    "            hidden_sizes=args.hidden_sizes,\n",
    "            device=args.device\n",
    "        ).to(args.device)\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "        agent_1 = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor=args.gamma,\n",
    "            action_space=env.action_space,\n",
    "            estimation_step=args.n_step,\n",
    "            target_update_freq=args.target_update_freq\n",
    "        )\n",
    "        if args.resume_path_1:\n",
    "            agent_1.load_state_dict(torch.load(args.resume_path_1))\n",
    "    \n",
    "    if agent_2 is None:\n",
    "        # model\n",
    "        net = Net(\n",
    "            args.state_shape,\n",
    "            args.action_shape,\n",
    "            hidden_sizes=args.hidden_sizes,\n",
    "            device=args.device\n",
    "        ).to(args.device)\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "        agent_2 = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor=args.gamma,\n",
    "            action_space=env.action_space,\n",
    "            estimation_step=args.n_step,\n",
    "            target_update_freq=args.target_update_freq\n",
    "        )\n",
    "        if args.resume_path_2:\n",
    "            agent_2.load_state_dict(torch.load(args.resume_path_2))\n",
    "    \n",
    "    agents = [agent_0, agent_1, agent_2]\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, optim, env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "        args: argparse.Namespace = get_args(),\n",
    "        agent_0: Optional[BasePolicy] = None,\n",
    "        agent_1: Optional[BasePolicy] = None,\n",
    "        agent_2: Optional[BasePolicy] = None,\n",
    "        optim: Optional[torch.optim.Optimizer] = None\n",
    ") -> Tuple[dict, BasePolicy]:\n",
    "    \n",
    "    # ======== environment setup ========\n",
    "    train_envs = DummyVectorEnv([get_env for _ in range(args.training_num)])\n",
    "    test_envs = DummyVectorEnv([get_env for _ in range(args.test_num)])\n",
    "    # seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    train_envs.seed(args.seed)\n",
    "    test_envs.seed(args.seed)\n",
    "\n",
    "    # ======== agent setup ========\n",
    "    policy, optim, agents = get_agents(args, agent_0=agent_0, agent_1=agent_1, agent_2=agent_2, optim=optim)\n",
    "\n",
    "    # ======== collector setup ========\n",
    "    train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(args.buffer_size, len(train_envs)),\n",
    "        exploration_noise=True\n",
    "    )\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "    # policy.set_eps(1)\n",
    "    train_collector.collect(n_step=args.batch_size * args.training_num)\n",
    "\n",
    "    # ======== tensorboard logging setup ========\n",
    "    log_path = os.path.join(args.logdir, 'simple_spread', 'dqn')\n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(\"args\", str(args))\n",
    "    logger = TensorboardLogger(writer)\n",
    "\n",
    "    # ======== callback functions used during training ========\n",
    "    def save_best_fn(policy):\n",
    "        if hasattr(args, 'model_save_path'):\n",
    "            model_save_path = args.model_save_path\n",
    "        else:\n",
    "            model_save_path = os.path.join(args.logdir, 'simple_spread', 'dqn', 'policy.pth')\n",
    "            torch.save(policy.policies[agents[args.agent_id - 1]].state_dict(), model_save_path)\n",
    "    \n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= args.win_rate\n",
    "    \n",
    "    def train_fn(epoch, env_step):\n",
    "        policy.policies[agents[args.agent_id - 2]].set_eps(args.eps_train)\n",
    "        policy.policies[agents[args.agent_id - 1]].set_eps(args.eps_train)\n",
    "        policy.policies[agents[args.agent_id]].set_eps(args.eps_train)\n",
    "    \n",
    "    def test_fn(epoch, env_step):\n",
    "        policy.policies[agents[args.agent_id - 2]].set_eps(args.eps_test)\n",
    "        policy.policies[agents[args.agent_id - 1]].set_eps(args.eps_test)\n",
    "        policy.policies[agents[args.agent_id]].set_eps(args.eps_test)\n",
    "    \n",
    "    def reward_metric(rews):\n",
    "        return (rews[:, args.agent_id - 2] + rews[:, args.agent_id - 1] + rews[:, args.agent_id]) / 3\n",
    "    \n",
    "    # trainer\n",
    "    result = OffpolicyTrainer(\n",
    "        policy,\n",
    "        train_collector,\n",
    "        test_collector,\n",
    "        args.epoch,\n",
    "        args.step_per_epoch,\n",
    "        args.step_per_collect,\n",
    "        args.test_num,\n",
    "        args.batch_size,\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        update_per_step=args.update_per_step,\n",
    "        logger=logger,\n",
    "        test_in_train=False,\n",
    "        reward_metric=reward_metric\n",
    "    ).run()\n",
    "\n",
    "    return result, policy.policies[agents[args.agent_id - 2]], policy.policies[agents[args.agent_id - 1]], policy.policies[agents[args.agent_id]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== a test function that tests a pre-trained agent ========\n",
    "def watch(\n",
    "        args: argparse.Namespace = get_args(),\n",
    "        agent_0: Optional[BasePolicy] = None,\n",
    "        agent_1: Optional[BasePolicy] = None,\n",
    "        agent_2: Optional[BasePolicy] = None\n",
    ") -> None:\n",
    "    env = get_env(render_mode=\"human\")\n",
    "    env = DummyVectorEnv([lambda: env])\n",
    "    policy, optim, agents = get_agents(args, agent_0=agent_0, agent_1=agent_1, agent_2=agent_2)\n",
    "    policy.eval()\n",
    "    policy.policies[agents[args.agent_id - 2]].set_eps(args.eps_test)\n",
    "    policy.policies[agents[args.agent_id - 1]].set_eps(args.eps_test)\n",
    "    policy.policies[agents[args.agent_id]].set_eps(args.eps_test)\n",
    "    collector = Collector(policy, env, exploration_noise=True)\n",
    "    result = collector.collect(n_episode=1, render=args.render)\n",
    "    rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "    print(f\"Final reward: {(rews[:, args.agent_id - 2] + rews[:, args.agent_id - 1] + rews[:, args.agent_id]) / 3}, length: {lens.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:   0%|          | 0/1000 [00:00<?, ?it/s]/Users/tanxiaoxuan/reinforcement_learning/RI_venv/lib/python3.10/site-packages/tianshou/data/collector.py:236: UserWarning: n_step=10 is not a multiple of #env (64), which may cause extra transitions collected into the buffer.\n",
      "  warnings.warn(\n",
      "Epoch #1:   6%|6         | 64/1000 [00:01<00:20, 46.51it/s, agent_0/loss=2.544, agent_1/loss=2.777, agent_2/loss=3.917, env_step=64, len=0, n/ep=0, n/st=64, rew=0.00]/Users/tanxiaoxuan/reinforcement_learning/RI_venv/lib/python3.10/site-packages/tianshou/data/collector.py:236: UserWarning: n_step=10 is not a multiple of #env (64), which may cause extra transitions collected into the buffer.\n",
      "  warnings.warn(\n",
      "Epoch #1: 1024it [00:02, 358.28it/s, agent_0/loss=1.947, agent_1/loss=3.895, agent_2/loss=4.931, env_step=1024, len=75, n/ep=0, n/st=64, rew=-39.62]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -36.370796 ± 9.416714, best_reward: -36.370796 ± 9.416714 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1024it [00:01, 670.07it/s, agent_0/loss=0.583, agent_1/loss=3.068, agent_2/loss=4.134, env_step=2048, len=75, n/ep=0, n/st=64, rew=-39.62]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -32.076792 ± 10.722334, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 1024it [00:01, 694.98it/s, agent_0/loss=0.210, agent_1/loss=1.519, agent_2/loss=1.842, env_step=3072, len=75, n/ep=0, n/st=64, rew=-39.62]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -36.222585 ± 8.337361, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 1024it [00:01, 613.86it/s, agent_0/loss=0.155, agent_1/loss=1.061, agent_2/loss=1.370, env_step=4096, len=75, n/ep=0, n/st=64, rew=-39.62]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -37.567739 ± 10.714771, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 1024it [00:01, 623.87it/s, agent_0/loss=0.127, agent_1/loss=1.367, agent_2/loss=1.799, env_step=5120, len=75, n/ep=0, n/st=64, rew=-39.62]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: -40.500537 ± 8.325558, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 1024it [00:01, 643.52it/s, agent_0/loss=0.114, agent_1/loss=1.542, agent_2/loss=1.922, env_step=6144, len=75, n/ep=0, n/st=64, rew=-29.56]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: -37.293571 ± 12.122750, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 1024it [00:01, 599.37it/s, agent_0/loss=0.131, agent_1/loss=1.230, agent_2/loss=1.600, env_step=7168, len=75, n/ep=0, n/st=64, rew=-29.56]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: -42.923744 ± 8.254176, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 1024it [00:01, 662.14it/s, agent_0/loss=0.129, agent_1/loss=1.065, agent_2/loss=1.340, env_step=8192, len=75, n/ep=0, n/st=64, rew=-29.56]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: -39.295618 ± 9.243113, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 1024it [00:01, 674.32it/s, agent_0/loss=0.130, agent_1/loss=1.332, agent_2/loss=1.781, env_step=9216, len=75, n/ep=0, n/st=64, rew=-29.56]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: -33.488854 ± 12.627411, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 1024it [00:01, 630.80it/s, agent_0/loss=0.130, agent_1/loss=1.591, agent_2/loss=1.968, env_step=10240, len=75, n/ep=0, n/st=64, rew=-29.56]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: -38.935602 ± 13.634608, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 1024it [00:01, 651.75it/s, agent_0/loss=0.127, agent_1/loss=1.488, agent_2/loss=1.953, env_step=11264, len=75, n/ep=0, n/st=64, rew=-30.42]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: -45.424167 ± 9.997725, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 1024it [00:01, 686.43it/s, agent_0/loss=0.134, agent_1/loss=1.138, agent_2/loss=1.536, env_step=12288, len=75, n/ep=0, n/st=64, rew=-30.42]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: -32.811949 ± 9.828154, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 1024it [00:01, 566.68it/s, agent_0/loss=0.158, agent_1/loss=1.248, agent_2/loss=1.622, env_step=13312, len=75, n/ep=0, n/st=64, rew=-30.42]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: -33.606169 ± 8.526111, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 1024it [00:01, 669.41it/s, agent_0/loss=0.137, agent_1/loss=1.890, agent_2/loss=2.475, env_step=14336, len=75, n/ep=0, n/st=64, rew=-30.42]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: -37.255410 ± 14.564766, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 1024it [00:01, 681.73it/s, agent_0/loss=0.127, agent_1/loss=2.492, agent_2/loss=3.134, env_step=15360, len=75, n/ep=0, n/st=64, rew=-35.13]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: -38.966974 ± 10.674187, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 1024it [00:01, 633.62it/s, agent_0/loss=0.117, agent_1/loss=2.039, agent_2/loss=2.606, env_step=16384, len=75, n/ep=0, n/st=64, rew=-35.13]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: -42.867950 ± 10.107353, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 1024it [00:01, 669.09it/s, agent_0/loss=0.120, agent_1/loss=1.150, agent_2/loss=1.535, env_step=17408, len=75, n/ep=0, n/st=64, rew=-35.13]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: -39.037031 ± 13.713885, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 1024it [00:01, 690.29it/s, agent_0/loss=0.123, agent_1/loss=1.475, agent_2/loss=1.812, env_step=18432, len=75, n/ep=0, n/st=64, rew=-35.13]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: -37.613538 ± 13.562409, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 1024it [00:01, 688.31it/s, agent_0/loss=0.201, agent_1/loss=2.191, agent_2/loss=2.750, env_step=19456, len=75, n/ep=0, n/st=64, rew=-35.13]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: -37.843118 ± 12.235490, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 1024it [00:01, 635.89it/s, agent_0/loss=0.128, agent_1/loss=2.552, agent_2/loss=3.075, env_step=20480, len=75, n/ep=0, n/st=64, rew=-35.32]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20: test_reward: -36.145667 ± 12.995778, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #21: 1024it [00:01, 696.45it/s, agent_0/loss=0.135, agent_1/loss=1.758, agent_2/loss=2.272, env_step=21504, len=75, n/ep=0, n/st=64, rew=-35.32]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #21: test_reward: -43.884089 ± 5.631914, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #22: 1024it [00:01, 682.04it/s, agent_0/loss=0.144, agent_1/loss=1.079, agent_2/loss=1.418, env_step=22528, len=75, n/ep=0, n/st=64, rew=-35.32]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #22: test_reward: -35.182916 ± 9.770252, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #23: 1024it [00:01, 642.79it/s, agent_0/loss=0.142, agent_1/loss=1.502, agent_2/loss=1.906, env_step=23552, len=75, n/ep=0, n/st=64, rew=-35.32]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #23: test_reward: -33.220257 ± 5.138335, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #24: 1024it [00:01, 697.42it/s, agent_0/loss=0.155, agent_1/loss=2.158, agent_2/loss=2.660, env_step=24576, len=75, n/ep=0, n/st=64, rew=-35.32]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #24: test_reward: -47.196802 ± 7.314344, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #25: 1024it [00:01, 680.24it/s, agent_0/loss=0.132, agent_1/loss=2.121, agent_2/loss=2.770, env_step=25600, len=75, n/ep=0, n/st=64, rew=-33.85]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #25: test_reward: -36.177803 ± 15.644656, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #26: 1024it [00:01, 684.63it/s, agent_0/loss=0.093, agent_1/loss=1.504, agent_2/loss=1.931, env_step=26624, len=75, n/ep=0, n/st=64, rew=-33.85]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #26: test_reward: -37.749935 ± 9.609524, best_reward: -32.076792 ± 10.722334 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #27: 1024it [00:01, 701.11it/s, agent_0/loss=0.083, agent_1/loss=1.022, agent_2/loss=1.309, env_step=27648, len=75, n/ep=0, n/st=64, rew=-33.85]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #27: test_reward: -28.657929 ± 9.817101, best_reward: -28.657929 ± 9.817101 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #28: 1024it [00:01, 667.34it/s, agent_0/loss=0.078, agent_1/loss=1.312, agent_2/loss=1.633, env_step=28672, len=75, n/ep=0, n/st=64, rew=-33.85]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #28: test_reward: -34.153730 ± 8.099020, best_reward: -28.657929 ± 9.817101 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #29: 1024it [00:01, 673.51it/s, agent_0/loss=0.108, agent_1/loss=1.581, agent_2/loss=2.061, env_step=29696, len=75, n/ep=0, n/st=64, rew=-29.90]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #29: test_reward: -31.425460 ± 6.504437, best_reward: -28.657929 ± 9.817101 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #30: 1024it [00:02, 500.11it/s, agent_0/loss=0.057, agent_1/loss=1.434, agent_2/loss=1.882, env_step=30720, len=75, n/ep=0, n/st=64, rew=-29.90]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #30: test_reward: -30.698508 ± 6.670738, best_reward: -28.657929 ± 9.817101 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #31: 1024it [00:01, 663.77it/s, agent_0/loss=0.096, agent_1/loss=1.077, agent_2/loss=1.463, env_step=31744, len=75, n/ep=0, n/st=64, rew=-29.90]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #31: test_reward: -35.799757 ± 6.993547, best_reward: -28.657929 ± 9.817101 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #32: 1024it [00:01, 671.10it/s, agent_0/loss=0.091, agent_1/loss=1.079, agent_2/loss=1.375, env_step=32768, len=75, n/ep=0, n/st=64, rew=-29.90]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #32: test_reward: -33.770466 ± 6.018041, best_reward: -28.657929 ± 9.817101 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #33: 1024it [00:02, 415.43it/s, agent_0/loss=0.099, agent_1/loss=1.434, agent_2/loss=1.800, env_step=33792, len=75, n/ep=0, n/st=64, rew=-29.90]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #33: test_reward: -28.840049 ± 5.356791, best_reward: -28.657929 ± 9.817101 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #34: 1024it [00:01, 570.07it/s, agent_0/loss=0.154, agent_1/loss=1.770, agent_2/loss=2.151, env_step=34816, len=75, n/ep=0, n/st=64, rew=-30.63]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #34: test_reward: -35.866943 ± 8.513604, best_reward: -28.657929 ± 9.817101 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #35: 1024it [00:01, 527.64it/s, agent_0/loss=0.085, agent_1/loss=1.333, agent_2/loss=1.788, env_step=35840, len=75, n/ep=0, n/st=64, rew=-30.63]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #35: test_reward: -24.939894 ± 5.713161, best_reward: -24.939894 ± 5.713161 in #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #36: 1024it [00:01, 637.65it/s, agent_0/loss=0.090, agent_1/loss=0.974, agent_2/loss=1.241, env_step=36864, len=75, n/ep=0, n/st=64, rew=-30.63]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #36: test_reward: -36.101970 ± 10.137590, best_reward: -24.939894 ± 5.713161 in #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #37: 1024it [00:01, 640.15it/s, agent_0/loss=0.115, agent_1/loss=1.125, agent_2/loss=1.397, env_step=37888, len=75, n/ep=0, n/st=64, rew=-30.63]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #37: test_reward: -34.008169 ± 9.349661, best_reward: -24.939894 ± 5.713161 in #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #38: 1024it [00:01, 586.31it/s, agent_0/loss=0.193, agent_1/loss=1.342, agent_2/loss=1.665, env_step=38912, len=75, n/ep=0, n/st=64, rew=-30.63]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #38: test_reward: -29.259032 ± 9.289847, best_reward: -24.939894 ± 5.713161 in #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #39: 1024it [00:01, 642.65it/s, agent_0/loss=0.115, agent_1/loss=1.272, agent_2/loss=1.664, env_step=39936, len=75, n/ep=0, n/st=64, rew=-28.18]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #39: test_reward: -35.611284 ± 8.291264, best_reward: -24.939894 ± 5.713161 in #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #40: 1024it [00:01, 604.65it/s, agent_0/loss=0.089, agent_1/loss=1.030, agent_2/loss=1.352, env_step=40960, len=75, n/ep=0, n/st=64, rew=-28.18]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #40: test_reward: -31.391037 ± 8.564477, best_reward: -24.939894 ± 5.713161 in #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #41: 1024it [00:01, 555.08it/s, agent_0/loss=0.090, agent_1/loss=0.935, agent_2/loss=1.247, env_step=41984, len=75, n/ep=0, n/st=64, rew=-28.18]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #41: test_reward: -31.896836 ± 10.691824, best_reward: -24.939894 ± 5.713161 in #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #42: 1024it [00:01, 632.57it/s, agent_0/loss=0.123, agent_1/loss=1.140, agent_2/loss=1.420, env_step=43008, len=75, n/ep=0, n/st=64, rew=-28.18]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #42: test_reward: -39.270934 ± 15.173113, best_reward: -24.939894 ± 5.713161 in #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #43: 1024it [00:01, 574.42it/s, agent_0/loss=0.290, agent_1/loss=1.263, agent_2/loss=1.622, env_step=44032, len=75, n/ep=0, n/st=64, rew=-28.19]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #43: test_reward: -35.439549 ± 8.012936, best_reward: -24.939894 ± 5.713161 in #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #44: 1024it [00:01, 626.89it/s, agent_0/loss=0.080, agent_1/loss=1.184, agent_2/loss=1.551, env_step=45056, len=75, n/ep=0, n/st=64, rew=-28.19]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #44: test_reward: -37.186627 ± 9.314452, best_reward: -24.939894 ± 5.713161 in #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #45: 1024it [00:01, 633.69it/s, agent_0/loss=0.072, agent_1/loss=1.013, agent_2/loss=1.342, env_step=46080, len=75, n/ep=0, n/st=64, rew=-28.19]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #45: test_reward: -30.362776 ± 11.817473, best_reward: -24.939894 ± 5.713161 in #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #46: 1024it [00:01, 600.86it/s, agent_0/loss=0.083, agent_1/loss=1.001, agent_2/loss=1.309, env_step=47104, len=75, n/ep=0, n/st=64, rew=-28.19]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #46: test_reward: -30.703335 ± 7.243411, best_reward: -24.939894 ± 5.713161 in #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #47: 1024it [00:01, 630.58it/s, agent_0/loss=0.110, agent_1/loss=1.200, agent_2/loss=1.563, env_step=48128, len=75, n/ep=0, n/st=64, rew=-28.19]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #47: test_reward: -23.694188 ± 5.183193, best_reward: -23.694188 ± 5.183193 in #47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #48: 1024it [00:01, 603.27it/s, agent_0/loss=0.060, agent_1/loss=1.451, agent_2/loss=1.901, env_step=49152, len=75, n/ep=0, n/st=64, rew=-29.60]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #48: test_reward: -33.997088 ± 6.836879, best_reward: -23.694188 ± 5.183193 in #47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #49: 1024it [00:01, 605.00it/s, agent_0/loss=0.074, agent_1/loss=1.371, agent_2/loss=1.777, env_step=50176, len=75, n/ep=0, n/st=64, rew=-29.60]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #49: test_reward: -35.012840 ± 11.617482, best_reward: -23.694188 ± 5.183193 in #47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #50: 1024it [00:01, 632.19it/s, agent_0/loss=0.073, agent_1/loss=1.089, agent_2/loss=1.410, env_step=51200, len=75, n/ep=0, n/st=64, rew=-29.60]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #50: test_reward: -36.164699 ± 8.399643, best_reward: -23.694188 ± 5.183193 in #47\n",
      "Final reward: [-52.150005], length: 75.0\n"
     ]
    }
   ],
   "source": [
    "# train the agent and watch its performance\n",
    "args = get_args()\n",
    "result, agent_0, agent_1, agent_2 = train_agent(args)\n",
    "watch(args, agent_0, agent_1, agent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env(render_mode=\"human\")\n",
    "env = DummyVectorEnv([lambda: env])\n",
    "policy, optim, agents = get_agents(args, agent_0=agent_0, agent_1=agent_1, agent_2=agent_2)\n",
    "policy.eval()\n",
    "policy.policies[agents[args.agent_id - 2]].set_eps(args.eps_test)\n",
    "policy.policies[agents[args.agent_id - 1]].set_eps(args.eps_test)\n",
    "policy.policies[agents[args.agent_id]].set_eps(args.eps_test)\n",
    "collector = Collector(policy, env, exploration_noise=True)\n",
    "result = collector.collect(n_episode=1, render=args.render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-27.57182694656241\n"
     ]
    }
   ],
   "source": [
    "eval_env = simple_spread_v3.env(\n",
    "    N=num_agents,\n",
    "    max_cycles=25,\n",
    "    local_ratio=0.5,\n",
    "    continuous_actions=False,\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tianshou.data import Batch\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ep_rewards = []\n",
    "num_episodes = 1\n",
    "for ep in range(num_episodes):\n",
    "    eval_env.reset(seed=42)\n",
    "    total_reward = {agent: 0 for agent in eval_env.agents}\n",
    "    for agent in eval_env.agent_iter():\n",
    "        observation, reward, termination, trunc, info = eval_env.last()\n",
    "        total_reward[agent] += reward\n",
    "        if (termination or trunc):\n",
    "            ep_rewards.append(total_reward[agent])\n",
    "            action = None\n",
    "        else:\n",
    "            batch = Batch(obs=[observation], info=info)\n",
    "            agent_id = int(str(agent).split('_')[1])\n",
    "            action = policy.policies[agents[agent_id]](batch).act[0]\n",
    "        eval_env.step(action)\n",
    "        img = eval_env.render()\n",
    "        plt.imshow(img)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "print(np.mean(ep_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe.simple_spread.simple_spread import Scenario, raw_env\n",
    "from gymnasium.utils import EzPickle\n",
    "from pettingzoo.mpe._mpe_utils.core import Agent, Landmark, World\n",
    "from pettingzoo.mpe._mpe_utils.scenario import BaseScenario\n",
    "from pettingzoo.mpe._mpe_utils.simple_env import SimpleEnv, make_env\n",
    "\n",
    "class Scenario_change_reward(Scenario):\n",
    "    def reward(self, agent, world):\n",
    "        rew = 0\n",
    "        if agent.collide:\n",
    "            for a in world.agents:\n",
    "                rew -= 2.0 * (self.is_collision(a, agent) and a != agent)\n",
    "        return rew\n",
    "    \n",
    "class raw_env_change_reward(raw_env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        N=3,\n",
    "        local_ratio=0.5,\n",
    "        max_cycles=25,\n",
    "        continuous_actions=False,\n",
    "        render_mode=None,\n",
    "    ):\n",
    "        EzPickle.__init__(\n",
    "            self,\n",
    "            N=N,\n",
    "            local_ratio=local_ratio,\n",
    "            max_cycles=max_cycles,\n",
    "            continuous_actions=continuous_actions,\n",
    "            render_mode=render_mode,\n",
    "        )\n",
    "        assert (\n",
    "            0.0 <= local_ratio <= 1.0\n",
    "        ), \"local_ratio is a proportion. Must be between 0 and 1.\"\n",
    "        scenario = Scenario_change_reward()  # change scenario\n",
    "        world = scenario.make_world(N)\n",
    "        SimpleEnv.__init__(\n",
    "            self,\n",
    "            scenario=scenario,\n",
    "            world=world,\n",
    "            render_mode=render_mode,\n",
    "            max_cycles=max_cycles,\n",
    "            continuous_actions=continuous_actions,\n",
    "            local_ratio=local_ratio,\n",
    "        )\n",
    "        self.metadata[\"name\"] = \"simple_spread_v3\"\n",
    "\n",
    "env_change_reward = make_env(raw_env_change_reward)\n",
    "env_change_reward = env_change_reward(N=3, max_cycles=25, local_ratio=0.5, continuous_actions=False, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
