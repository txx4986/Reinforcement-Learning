{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Push\n",
    "**Objectives**: The environment has 1 good agent, 1 adversary and 1 landmark. The good agent is rewarded based on the distance to the landmark. The adversary is rewarded if it is close to the landmark, and if the agent is far from the landmark (the difference of the distances). Thus, the adversary must learn to push the good agent away from the landmark. <br>\n",
    "\n",
    "**Actions**: \n",
    "- Agent observation space: `[self_vel, goal_rel_position, goal_landmark_id, all_landmark_rel_positions, landmark_ids, other_agent_rel_positions]`\n",
    "- Adversary observation space: `[self_vel, all_landmark_rel_positions, other_agent_rel_positions]`\n",
    "- Agent action space: `[no_action, move_left, move_right, move_down, move_up]`\n",
    "- Adversary action space: `[no_action, move_left, move_right, move_down, move_up]`\n",
    "\n",
    "**Approach**: use Tianshou to train DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_push_v3\n",
    "env = simple_push_v3.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action = env.action_space(agent).sample()\n",
    "    \n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-28 14:48:52.258319: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "from pettingzoo.mpe import simple_push_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_env():\n",
    "    \"\"\"This functions is needed to provide callables for DummyVectorEnv\"\"\"\n",
    "    return PettingZooEnv(simple_push_v3.env())\n",
    "\n",
    "def _get_agents(\n",
    "    agent: Optional[BasePolicy] = None,\n",
    "    adversary: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    env = _get_env()\n",
    "    observation_space = (\n",
    "        env.observation_space[\"observation\"]\n",
    "        if isinstance(env.observation_space, gymnasium.spaces.Dict)\n",
    "        else env.observation_space\n",
    "    )\n",
    "    if agent is None:\n",
    "        # model\n",
    "        net = Net(\n",
    "            state_shape=observation_space.shape or observation_space.n,\n",
    "            action_shape=env.action_space.shape or env.action_space.n,\n",
    "            hidden_sizes=[128, 128, 128, 128],\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "        agent = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor=0.9,\n",
    "            estimation_step=3,\n",
    "            target_update_freq=320\n",
    "        )\n",
    "    if adversary is None:\n",
    "        adversary = RandomPolicy()\n",
    "    \n",
    "    agents = [adversary, agent]\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, optim, env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/tanxiaoxuan/reinforcement_learning/Reinforcement-Learning/Simple_Push.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanxiaoxuan/reinforcement_learning/Reinforcement-Learning/Simple_Push.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanxiaoxuan/reinforcement_learning/Reinforcement-Learning/Simple_Push.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# step 1: environment setup\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tanxiaoxuan/reinforcement_learning/Reinforcement-Learning/Simple_Push.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_envs \u001b[39m=\u001b[39m DummyVectorEnv([_get_env \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39m10\u001b[39;49m)])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanxiaoxuan/reinforcement_learning/Reinforcement-Learning/Simple_Push.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     test_envs \u001b[39m=\u001b[39m DummyVectorEnv([_get_env \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m)])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanxiaoxuan/reinforcement_learning/Reinforcement-Learning/Simple_Push.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# seed\u001b[39;00m\n",
      "File \u001b[0;32m~/reinforcement_learning/RI_venv/lib/python3.10/site-packages/tianshou/env/venvs.py:443\u001b[0m, in \u001b[0;36mDummyVectorEnv.__init__\u001b[0;34m(self, env_fns, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, env_fns: List[Callable[[], ENV_TYPE]], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(env_fns, DummyEnvWorker, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/reinforcement_learning/RI_venv/lib/python3.10/site-packages/tianshou/env/venvs.py:153\u001b[0m, in \u001b[0;36mBaseVectorEnv.__init__\u001b[0;34m(self, env_fns, worker_fn, wait_num, timeout)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_env_fns \u001b[39m=\u001b[39m env_fns\n\u001b[1;32m    151\u001b[0m \u001b[39m# A VectorEnv contains a pool of EnvWorkers, which corresponds to\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m# interact with the given envs (one worker <-> one env).\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m [worker_fn(_patch_env_generator(fn)) \u001b[39mfor\u001b[39;00m fn \u001b[39min\u001b[39;00m env_fns]\n\u001b[1;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworker_class \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers[\u001b[39m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworker_class, EnvWorker)\n",
      "File \u001b[0;32m~/reinforcement_learning/RI_venv/lib/python3.10/site-packages/tianshou/env/venvs.py:153\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_env_fns \u001b[39m=\u001b[39m env_fns\n\u001b[1;32m    151\u001b[0m \u001b[39m# A VectorEnv contains a pool of EnvWorkers, which corresponds to\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m# interact with the given envs (one worker <-> one env).\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m [worker_fn(_patch_env_generator(fn)) \u001b[39mfor\u001b[39;00m fn \u001b[39min\u001b[39;00m env_fns]\n\u001b[1;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworker_class \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers[\u001b[39m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworker_class, EnvWorker)\n",
      "File \u001b[0;32m~/reinforcement_learning/RI_venv/lib/python3.10/site-packages/tianshou/env/worker/dummy.py:13\u001b[0m, in \u001b[0;36mDummyEnvWorker.__init__\u001b[0;34m(self, env_fn)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, env_fn: Callable[[], gym\u001b[39m.\u001b[39mEnv]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env_fn()\n\u001b[1;32m     14\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(env_fn)\n",
      "File \u001b[0;32m~/reinforcement_learning/RI_venv/lib/python3.10/site-packages/tianshou/env/venvs.py:48\u001b[0m, in \u001b[0;36m_patch_env_generator.<locals>.patched\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpatched\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m gym\u001b[39m.\u001b[39mEnv:\n\u001b[1;32m     44\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(\n\u001b[1;32m     45\u001b[0m         fn\n\u001b[1;32m     46\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mEnv generators that are provided to vector environemnts must be callable.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m     env \u001b[39m=\u001b[39m fn()\n\u001b[1;32m     49\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(env, (gym\u001b[39m.\u001b[39mEnv, PettingZooEnv)):\n\u001b[1;32m     50\u001b[0m         \u001b[39mreturn\u001b[39;00m env\n",
      "\u001b[1;32m/Users/tanxiaoxuan/reinforcement_learning/Reinforcement-Learning/Simple_Push.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanxiaoxuan/reinforcement_learning/Reinforcement-Learning/Simple_Push.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_env\u001b[39m():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanxiaoxuan/reinforcement_learning/Reinforcement-Learning/Simple_Push.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This functions is needed to provide callables for DummyVectorEnv\"\"\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tanxiaoxuan/reinforcement_learning/Reinforcement-Learning/Simple_Push.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m PettingZooEnv(simple_push_v3\u001b[39m.\u001b[39;49menv())\n",
      "File \u001b[0;32m~/reinforcement_learning/RI_venv/lib/python3.10/site-packages/tianshou/env/pettingzoo_env.py:54\u001b[0m, in \u001b[0;36mPettingZooEnv.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Get first action space, assuming all agents have equal space\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space: Any \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents[\u001b[39m0\u001b[39m])\n\u001b[0;32m---> 54\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mobservation_space(agent) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\n\u001b[1;32m     55\u001b[0m            \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents), \\\n\u001b[1;32m     56\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mObservation spaces for all agents must be identical. Perhaps \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m     57\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSuperSuit\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms pad_observations wrapper can help (useage: \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m     58\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`supersuit.aec_wrappers.pad_observations(env)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space(agent) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\n\u001b[1;32m     61\u001b[0m            \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents), \\\n\u001b[1;32m     62\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mAction spaces for all agents must be identical. Perhaps \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m     63\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSuperSuit\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms pad_action_space wrapper can help (useage: \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m     64\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`supersuit.aec_wrappers.pad_action_space(env)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n",
      "\u001b[0;31mAssertionError\u001b[0m: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # step 1: environment setup\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(10)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(10)])\n",
    "\n",
    "    # seed\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # step 2: agent setup\n",
    "    policy, optim, agents = _get_agents()\n",
    "\n",
    "    # step 3: collector setup\n",
    "    train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(20_000, len(train_envs)),\n",
    "        exploration_noise=True\n",
    "    )\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "    # policy.set_eps(1)\n",
    "    train_collector.collect(n_step=64 * 10)  # batch size * training_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "from pettingzoo.mpe import simple_push_v3\n",
    "\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    env = _get_env()\n",
    "    observation_space = (\n",
    "        env.observation_space[\"observation\"]\n",
    "        if isinstance(env.observation_space, gymnasium.spaces.Dict)\n",
    "        else env.observation_space\n",
    "    )\n",
    "    if agent_learn is None:\n",
    "        # model\n",
    "        net = Net(\n",
    "            state_shape=observation_space.shape or observation_space.n,\n",
    "            action_shape=env.action_space.shape or env.action_space.n,\n",
    "            hidden_sizes=[128, 128, 128, 128],\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "        agent_learn = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor=0.9,\n",
    "            estimation_step=3,\n",
    "            target_update_freq=320\n",
    "        )\n",
    "    if agent_opponent is None:\n",
    "        agent_opponent = RandomPolicy()\n",
    "    \n",
    "    agents = [agent_opponent, agent_learn]\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, optim, env.agents\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This functions is needed to provide callables for DummyVectorEnv\"\"\"\n",
    "    return PettingZooEnv(tictactoe_v3.env())\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # step 1: environment setup\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(10)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(10)])\n",
    "\n",
    "    # seed\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # step 2: agent setup\n",
    "    policy, optim, agents = _get_agents()\n",
    "\n",
    "    # step 3: collector setup\n",
    "    train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(20_000, len(train_envs)),\n",
    "        exploration_noise=True\n",
    "    )\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "    # policy.set_eps(1)\n",
    "    train_collector.collect(n_step=64 * 10)  # batch size * training_num\n",
    "\n",
    "# step 4: callback functions setup\n",
    "def save_best_fn(policy):\n",
    "    model_save_path = os.path.join(\"log\", \"ttt\", \"dqn\", \"policy.pth\")\n",
    "    os.makedirs(os.path.join(\"log\", \"ttt\", \"dqn\"), exist_ok=True)\n",
    "    torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "\n",
    "def stop_fn(mean_rewards):\n",
    "    return mean_rewards >= 0.6\n",
    "\n",
    "def train_fn(epoch, env_step):\n",
    "    policy.policies[agents[1]].set_eps(0.1)\n",
    "\n",
    "def test_fn(epoch, env_step):\n",
    "    policy.policies[agents[1]].set_eps(0.05)\n",
    "\n",
    "def reward_metric(rews):\n",
    "    return rews[:, 1]\n",
    "\n",
    "# step 5: run the trainer\n",
    "result = offpolicy_trainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=50,\n",
    "    step_per_epoch=1000,\n",
    "    step_per_collect=50,\n",
    "    episode_per_test=10,\n",
    "    batch_size=64,\n",
    "    train_fn=train_fn,\n",
    "    test_fn=test_fn,\n",
    "    stop_fn=stop_fn,\n",
    "    save_best_fn=save_best_fn,\n",
    "    update_per_step=0.1,\n",
    "    test_in_train=False,\n",
    "    reward_metric=reward_metric\n",
    ")\n",
    "\n",
    "# return result, policy.policies[agents[1]]\n",
    "print(f\"\\n==========Result==========\\n{result}\")\n",
    "print(\"\\n(the trained policy can be accessed via policy.policies[agents[1]])\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
