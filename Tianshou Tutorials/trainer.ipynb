{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "## Overview\n",
    "Trainer is the highest-level encapsulation in Tianshou. It controls the training loop and the evaluation method. It also controls the interaction between the Collector and the Policy, with the Replay Buffer serving as the media.\n",
    "\n",
    "## Usages\n",
    "There are three types of Trainer, designed to be used in on-policy training, off-policy training and offline training respectively.\n",
    "\n",
    "### Training without trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.policy import PGPolicy\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.discrete import Actor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_env_num = 4\n",
    "buffer_size = 2000  # since REINFORCE is an on-policy algoritm, we don't need a very large buffer size\n",
    "\n",
    "# create the environments, used for training and evaluation\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "test_envs = DummyVectorEnv([lambda: gym.make(\"CartPole-v0\") for _ in range(2)])\n",
    "train_envs = DummyVectorEnv([lambda: gym.make(\"CartPole-v0\") for _ in range(train_env_num)])\n",
    "\n",
    "# create the policy instance\n",
    "net = Net(env.observation_space.shape, hidden_sizes=[16,])\n",
    "actor = Actor(net, env.action_space.shape)\n",
    "optim = torch.optim.Adam(actor.parameters(), lr=0.001)\n",
    "policy = PGPolicy(actor, optim, dist_fn=torch.distributions.Categorical)\n",
    "\n",
    "# create the replay buffer and the collector\n",
    "replaybuffer = VectorReplayBuffer(buffer_size, train_env_num)\n",
    "test_collector = Collector(policy, test_envs)\n",
    "train_collector = Collector(policy, train_envs, replaybuffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try training our policy network. The logic is simple. We collect some data into the buffer and then we use the data to train our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalaution reward is 9.8\n",
      "Evalaution reward is 9.3\n",
      "Evalaution reward is 9.2\n",
      "Evalaution reward is 9.8\n",
      "Evalaution reward is 9.3\n",
      "Evalaution reward is 9.6\n",
      "Evalaution reward is 8.8\n",
      "Evalaution reward is 9.4\n",
      "Evalaution reward is 9.3\n",
      "Evalaution reward is 9.0\n"
     ]
    }
   ],
   "source": [
    "train_collector.reset()\n",
    "train_envs.reset()\n",
    "test_collector.reset()\n",
    "test_envs.reset()\n",
    "replaybuffer.reset()\n",
    "for i in range(10):\n",
    "    evaluation_result = test_collector.collect(n_episode=10)\n",
    "    print(\"Evalaution reward is {}\". format(evaluation_result[\"rew\"]))\n",
    "    train_collector.collect(n_step=2000)\n",
    "    # 0 means taking all data stored in train_collector.buffer\n",
    "    policy.update(0, train_collector.buffer, batch_size=512, repeat=1)\n",
    "    train_collector.reset_buffer(keep_statistics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation reward doesn't seem to improve. That is simply because we haven't trained it for enough time. Plus, the network size is too small and REINFORCE algorithm is actually not very stable.\n",
    "\n",
    "### Training with trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 2000it [00:00, 3811.17it/s, env_step=2000, len=9, loss=0.000, n/ep=213, n/st=2000, rew=9.37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 9.400000 ± 0.489898, best_reward: 9.500000 ± 0.500000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #2: 2000it [00:00, 4509.43it/s, env_step=4000, len=9, loss=0.000, n/ep=211, n/st=2000, rew=9.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 9.000000 ± 0.632456, best_reward: 9.500000 ± 0.500000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #3: 2000it [00:00, 5348.88it/s, env_step=6000, len=9, loss=0.000, n/ep=213, n/st=2000, rew=9.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 9.400000 ± 0.663325, best_reward: 9.500000 ± 0.500000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #4: 2000it [00:00, 5575.69it/s, env_step=8000, len=9, loss=0.000, n/ep=214, n/st=2000, rew=9.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 9.700000 ± 0.458258, best_reward: 9.700000 ± 0.458258 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #5: 2000it [00:00, 5656.60it/s, env_step=10000, len=9, loss=0.000, n/ep=212, n/st=2000, rew=9.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 8.900000 ± 0.830662, best_reward: 9.700000 ± 0.458258 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #6: 2000it [00:00, 5658.24it/s, env_step=12000, len=9, loss=0.000, n/ep=215, n/st=2000, rew=9.30]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 9.500000 ± 0.500000, best_reward: 9.700000 ± 0.458258 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #7: 2000it [00:00, 5352.18it/s, env_step=14000, len=9, loss=0.000, n/ep=212, n/st=2000, rew=9.45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 9.300000 ± 0.640312, best_reward: 9.700000 ± 0.458258 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #8: 2000it [00:00, 5411.20it/s, env_step=16000, len=9, loss=0.000, n/ep=215, n/st=2000, rew=9.31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 9.400000 ± 0.800000, best_reward: 9.700000 ± 0.458258 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #9: 2000it [00:00, 4629.43it/s, env_step=18000, len=9, loss=0.000, n/ep=214, n/st=2000, rew=9.38]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 9.200000 ± 0.748331, best_reward: 9.700000 ± 0.458258 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #10: 2000it [00:00, 5454.67it/s, env_step=20000, len=9, loss=0.000, n/ep=212, n/st=2000, rew=9.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 9.100000 ± 0.538516, best_reward: 9.700000 ± 0.458258 in #4\n",
      "{'duration': '4.44s', 'train_time/model': '0.19s', 'test_step': 1024, 'test_episode': 110, 'test_time': '0.42s', 'test_speed': '2465.46 step/s', 'best_reward': 9.7, 'best_result': '9.70 ± 0.46', 'train_step': 20000, 'train_episode': 2131, 'train_time/collector': '3.84s', 'train_speed': '4963.75 step/s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tianshou.trainer import onpolicy_trainer\n",
    "\n",
    "train_collector.reset()\n",
    "train_envs.reset()\n",
    "test_collector.reset()\n",
    "test_envs.reset()\n",
    "replaybuffer.reset()\n",
    "\n",
    "result = onpolicy_trainer(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    max_epoch=10,\n",
    "    step_per_epoch=1,\n",
    "    repeat_per_collect=1,\n",
    "    episode_per_test=10,\n",
    "    step_per_collect=2000,\n",
    "    batch_size=512\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
