{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Network\n",
    "## Overview\n",
    "In reinforcement learning, the agent interacts with environments to improve itself.\n",
    "\n",
    "There are three types of data flow in RL training pipeline:\n",
    "1. Agent to environment: `action` will be generated by agent and sent to environment\n",
    "2. Environment to agent: `env.step` takes action, and returns a tuple of `(observation, reward, terminated, truncated, info)`\n",
    "3. Agent-environment interaction to agent training: the data generated by interaction will be stored and sent to the learner of agent\n",
    "\n",
    "In the following sections, we will set up (vectorised) environments, policy (with neural network), collector (with buffer), and trainer to successfully run th RL training and evaluation pipeline.\n",
    "\n",
    "## Make an Environment\n",
    "First of all, you have to make an environment for your agent to interact with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanxiaoxuan/reinforcement_learning/RI_venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:523: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import tianshou as ts\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vectorized Environment\n",
    "Tianshou supports vectorized environment for all algorithms. It provides four types of vectorized environment wrapper:\n",
    "- `DummyVectorEnv`: the sequential version, using a single-thread for-loop\n",
    "- `SubprocVectorEnv`: use python multiprocessing and pipe for concurrent execution\n",
    "- `ShmemVectorEnv`: use share memory instead of pipe based on SubprocVectorEnv\n",
    "- `RayVectorEnv`: use Ray for concurrent activities and is currently the only choice for parallel simulation in a cluster with multiple machines\n",
    "\n",
    "Here we set up 10 environments in `train_envs` and 100 environments in `test_envs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanxiaoxuan/reinforcement_learning/RI_venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:523: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "train_envs = ts.env.DummyVectorEnv([lambda: gym.make('CartPole-v0') for _ in range(10)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: gym.make('CartPole-v0') for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Network\n",
    "Tianshou supports any user-defined PyTorch networks and optimizers. Yet, of course, the inputs and outputs must comply with Tianshou's API.\n",
    "\n",
    "You can also use pre-defined MLP networks in `common`, `discrete` and `continuous`. The rules of self-defined networks are:\n",
    "1. Input: observation `obs` (may be a `numpy.ndarray`, `torch.Tensor`, dict, or self-defined class), hidden state `state` (for RNN usage), and other information `info` provided by the environment\n",
    "2. Output: some `logits`, the next hidden state `state`. The logits could be a tuple instead of a `torch.Tensor`, or some other useful variables or results during the policy forwarding procedure. It depends on how the policy class process the network ouput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape))\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "        batch = obs.shape[0]\n",
    "        logits = self.model(obs.view(batch, -1))\n",
    "        return logits, state\n",
    "    \n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "net = Net(state_shape, action_shape)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Policy\n",
    "We use the defined `net` and `optim` above, with extra policy hyper-parameters to define a policy. Here we define a DQN policy with target network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net,\n",
    "    optim=optim,\n",
    "    action_space=env.action_space,\n",
    "    discount_factor=0.9,\n",
    "    estimation_step=3,\n",
    "    target_update_freq=320\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Collector\n",
    "The collector is a key concept in Tianshou. It allows the policy to interact with different types of environments conveniently. In each step, the collector will let the policy perform (at least) a specified number of steps or episodes and store the data in a replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(20000, 10), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function of collector is the collector function, which can be summarised in the following lines:\n",
    "\n",
    "result = self.policy(self.data, last_state)  # the agent predicts the batch action from batch observation\n",
    "\n",
    "act = to_numpy(result.act)\n",
    "\n",
    "self.data.update(act=act)  # update the data with new action/policy\n",
    "\n",
    "result = self.env.step(act, ready_env_ids)  # apply action to environment\n",
    "\n",
    "obs_next, rew, done, info = result\n",
    "\n",
    "self.data.update(obs_next=obs_next, rew=rew, done=done, info=info)  # update the data with new state/reward/done/info\n",
    "\n",
    "## Train Policy with a Trainer\n",
    "Tianshou provides `OnPolicyTrainer`, `OffPolicyTrainer` and `OfflineTrainer`. The trainer will automatically stop training when the policy reaches the stop condition `stop_fn` on test collector. Since DQN is an off-policy algorithm, we use the `OffPolicyTrainer` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:   4%|4         | 450/10000 [00:03<01:23, 113.94it/s, env_step=450, len=200, n/ep=1, n/st=10, rew=200.00]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training! Use 4.83s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.OffpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=10, step_per_epoch=10000, step_per_collect=10,\n",
    "    update_per_step=0.1, episode_per_test=100, batch_size=64,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold,\n",
    "    logger=logger\n",
    ").run()\n",
    "print(f'Finished training! Use {result[\"duration\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of each parameter is as follows:\n",
    "- `max_epoch`: the maximum of epochs for training. the training process might be finished before reaching the `max_epoch`\n",
    "- `step_per_epoch`: the number of environment step (transition) collected per epoch\n",
    "- `step_per_collect`: the number of transition the collector would collect before the network update. For example, the code above means \"collect 10 transitions and do one policy network update\"\n",
    "- `episode_per_test`: the number of episodes for one policy evaluation\n",
    "- `batch_size`: the batch size of sample data, which is going to feed in the policy network\n",
    "- `train_fn`: a function receives the current number of epoch and step index, and performs some operations at the beginning of training in this epoch. For example, the code above means \"reset the epsilon to 0.1 in DQN before training\"\n",
    "- `test_fn`: a function receives the current number of epoch and step index, and performs some operations at the beginning of testing in this epoch. For example, the code above means \"reset the epsilon to 0.05 in DQN before testing\"\n",
    "- `stop_fn`: a function receives the average undiscounted returns of the testing result, return a boolean which indicates whether reaching the goal\n",
    "- `logger`: see below\n",
    "\n",
    "The trainer supports `TensorBoard` for logging. It can be used as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "writer = SummaryWriter('log/dqn')\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the logger into the trainer, and the training result will be recorded into the TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duration': '4.83s', 'train_time/model': '0.39s', 'test_step': 111047, 'test_episode': 600, 'test_time': '4.32s', 'test_speed': '25729.33 step/s', 'best_reward': 195.31, 'best_result': '195.31 Â± 10.28', 'train_step': 450, 'train_episode': 5, 'train_time/collector': '0.12s', 'train_speed': '876.59 step/s'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that within 4.83 seconds, we finished training a DQN agent on CartPole. The mean returns over 100 consercutive episodes is 195.31.\n",
    "\n",
    "## Save/Load Policy\n",
    "Since the policy inherits the class `torch.nn.Module`, saving and loading the policy are exactly the same as a torch module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(policy.state_dict(), 'dqn.pth')\n",
    "policy.load_state_dict(torch.load('dqn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the Agent's Performance\n",
    "`Collector` supports rendering. Here is the example of wtaching the agent's performance in 35 FPS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanxiaoxuan/reinforcement_learning/RI_venv/lib/python3.10/site-packages/tianshou/data/collector.py:68: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n",
      "/Users/tanxiaoxuan/reinforcement_learning/RI_venv/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:215: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n/ep': 1,\n",
       " 'n/st': 200,\n",
       " 'rews': array([200.]),\n",
       " 'lens': array([200]),\n",
       " 'idxs': array([0]),\n",
       " 'rew': 200.0,\n",
       " 'len': 200.0,\n",
       " 'rew_std': 0.0,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(0.05)\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "collector.collect(n_episode=1, render=1 / 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to manually see the action generated by a well-trained agent:\n",
    "\n",
    "assume obs is a single environment observation\n",
    "\n",
    "action = policy(Batch(obs=np.array([obs]))).act[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Policy with Customised Codes\n",
    "Tianshou supports user-defined training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tx/t6fnblk14z98ct88jxg69ctm0000gn/T/ipykernel_52827/42382888.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  if collect_result['rews'].mean() >= env.spec.reward_threshold or i % 1000 == 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training! Test mean returns: 199.59\n"
     ]
    }
   ],
   "source": [
    "# pre-collect at least 5000 transitions with random action before training\n",
    "train_collector.collect(n_step=5000, random=True)\n",
    "\n",
    "policy.set_eps(0.1)\n",
    "for i in range(int(1e6)):  # total step\n",
    "    collect_result = train_collector.collect(n_step=10)\n",
    "\n",
    "    # once if the collected episodes' mean returns reach the threshold, or every 1000 steps, we test it on test_collector\n",
    "    if collect_result['rews'].mean() >= env.spec.reward_threshold or i % 1000 == 0:\n",
    "        policy.set_eps(0.05)\n",
    "        result = test_collector.collect(n_episode=100)\n",
    "        if result['rews'].mean() >= env.spec.reward_threshold:\n",
    "            print(f'Finished training! Test mean returns: {result[\"rews\"].mean()}')\n",
    "            break\n",
    "        else:\n",
    "            # back to training eps\n",
    "            policy.set_eps(0.1)\n",
    "    \n",
    "    # train policy with a sampled batch data from buffer\n",
    "    losses = policy.update(64, train_collector.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
