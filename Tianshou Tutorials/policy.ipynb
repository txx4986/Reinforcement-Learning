{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy\n",
    "## Overview\n",
    "In Tianshou, both the agent and the core DRL algorithm are implemented in the Policy module. Tianshou provides more than 20 Policy modules, each representing one DRL algorithm. All Policy modules inherit from a BasePolicy Class and share the same interface.\n",
    "\n",
    "## Creating you own Policy\n",
    "We will use the simple REINFORCE algorithm Policy to show the implementation of a Policy Module.\n",
    "\n",
    "### Initialisation\n",
    "Firstly we create the `REINFORCEPolicy` by inheriting `BasePolicy` in Tianshou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Type, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Batch, ReplayBuffer, to_torch, to_torch_as\n",
    "from tianshou.policy import BasePolicy\n",
    "\n",
    "class REINFORCEPolicy(BasePolicy):\n",
    "    \"\"\"Implementation of REINFORCE algorithm\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Policy Module mainly does two things:\n",
    "1. `policy.forward()` receives observation and other information (stored in a Batch) from the environment and returns a new Batch containing the action\n",
    "2. `policy.update()` receives training data sampled from the replay buffer and updates itself, and then returns logging details\n",
    "\n",
    "We also need to take care of the following things:\n",
    "1. Since Tianshou is a Deep RL library, there should be a policy network in out Policy Module, also a Torch optimiser\n",
    "2. In Tianshou's BasePolicy, `policy.update()` first calls `Policy.process_fn()` to preprocess training data and computes quantities like episodic returns (gradient free), then it will call `Policy.learn() to perform the back-propagation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import numpy as np\n",
    "from tianshou.data import Batch, ReplayBuffer\n",
    "\n",
    "\n",
    "class REINFORCEPolicy(BasePolicy):\n",
    "    \"\"\"Implementation of REINFORCE algorithm\"\"\"\n",
    "    def __init__(self, model: torch.nn.Module, optim: torch.optim.Optimizer):\n",
    "        super().__init__()\n",
    "        self.actor = model\n",
    "        self.optim = optim\n",
    "    \n",
    "    def forward(self, batch: Batch) -> Batch:\n",
    "        \"\"\"Compute action over the given batch data\"\"\"\n",
    "        act = None\n",
    "        return Batch(act=act)\n",
    "    \n",
    "    def process_fn(self, batch: Batch, buffer: ReplayBuffer, indices: np.ndarray) -> Batch:\n",
    "        \"\"\"compute the discounted returns for each trastion\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self, batch: Batch, batch_size: int, repeat: int) -> Dict[str, List[float]]:\n",
    "        \"\"\"perform the back-propagation\"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy.forward()\n",
    "According to the equation of REINFORCE algorithm in Spinning Up's documentation, we need to map the observation to an action distribution in action space using neural network (`self.actor`)\n",
    "$$\\hat{g}=\\frac{1}{|D|}\\sum_{\\tau\\in D}{\\sum_{t=0}^{T}{\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})R(\\tau)}}$$\n",
    "Let us suppose the action space is discrete, and the distribution is a simple categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, batch: Batch) -> Batch:\n",
    "    \"\"\"compute action over the given bacth data\"\"\"\n",
    "    self.dist_fn = torch.distributions.Categorical\n",
    "    logits = self.actor(batch.obs)\n",
    "    dist = self.dist_fn(logits)\n",
    "    act = dist.sample()\n",
    "    return Batch(act=act, dist=dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy.process_fn()\n",
    "Now that we have defined out actor, if given training data we can set up a loss function and optimise our neural network. However, before that we must first calculate episodic returns for every step in out training data to construct the REINFORCE loss function.\n",
    "\n",
    "Calculating episodeic return is not hard, given `ReplayBuffer.next()` allows us to access every reward to go in an episode. A more convenient way would be to simply use the built-in method `BasePolicy.compute_episodic_return` inherited from BasePolicy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fn(self, batch: Batch, buffer: ReplayBuffer, indices: np.ndarray) -> Batch:\n",
    "    \"\"\"compute the discounted returns for each transition\"\"\"\n",
    "    returns, _ = self.compute_episodic_return(batch, buffer, indices, gamma=0.99, gae_lambda=1.0)\n",
    "    batch.returns = returns\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BasePolicy.compute_episodic_return()` could also be used to compute GAE. Another similar method is `BasePolicy. compute_nstep_return()`.\n",
    "\n",
    "### Policy.learn()\n",
    "Data batch returned by `Policy.process_fn` will flow into `Policy.learn()`. Finally we can construct our loss function and perform the back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, batch: Batch, batch_size: int, repeat: int) -> Dict[str, List[float]]:\n",
    "    \"\"\"perform the back-propagation\"\"\"\n",
    "    logging_losses = []\n",
    "    for _ in range(repeat):\n",
    "        for minibatch in batch.split(batch_size, merge_last=True):\n",
    "            self.optim.zero_grad()\n",
    "            result = self(minibatch)\n",
    "            dist = result.dist\n",
    "            act = to_torch_as(minibatch.act, result.act)\n",
    "            ret = to_torch(minibatch.returns, torch.float, result.act.device)\n",
    "            log_prob = dist.log_prob(act).reshape(len(ret), -1).transpose(0, 1)\n",
    "            loss = -(log_prob * ret).mean()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "            logging_losses.append(loss.item())\n",
    "    return {\"loss\": logging_losses}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import numpy as np\n",
    "from tianshou.data import Batch, ReplayBuffer\n",
    "\n",
    "\n",
    "class REINFORCEPolicy(BasePolicy):\n",
    "    \"\"\"Implementation of REINFORCE algorithm\"\"\"\n",
    "    def __init__(self, model: torch.nn.Module, optim: torch.optim.Optimizer):\n",
    "        super().__init__()\n",
    "        self.actor = model\n",
    "        self.optim = optim\n",
    "        self.dist_fn = torch.distributions.Categorical\n",
    "\n",
    "    def forward(self, batch: Batch) -> Batch:\n",
    "        \"\"\"compute action over the given bacth data\"\"\"\n",
    "        logits, _ = self.actor(batch.obs)\n",
    "        dist = self.dist_fn(logits)\n",
    "        act = dist.sample()\n",
    "        return Batch(act=act, dist=dist)\n",
    "    \n",
    "    def process_fn(self, batch: Batch, buffer: ReplayBuffer, indices: np.ndarray) -> Batch:\n",
    "        \"\"\"compute the discounted returns for each transition\"\"\"\n",
    "        returns, _ = self.compute_episodic_return(batch, buffer, indices, gamma=0.99, gae_lambda=1.0)\n",
    "        batch.returns = returns\n",
    "        return batch\n",
    "    \n",
    "    def learn(self, batch: Batch, batch_size: int, repeat: int) -> Dict[str, List[float]]:\n",
    "        \"\"\"perform the back-propagation\"\"\"\n",
    "        logging_losses = []\n",
    "        for _ in range(repeat):\n",
    "            for minibatch in batch.split(batch_size, merge_last=True):\n",
    "                self.optim.zero_grad()\n",
    "                result = self(minibatch)\n",
    "                dist = result.dist\n",
    "                act = to_torch_as(minibatch.act, result.act)\n",
    "                ret = to_torch(minibatch.returns, torch.float, result.act.device)\n",
    "                log_prob = dist.log_prob(act).reshape(len(ret), -1).transpose(0, 1)\n",
    "                loss = -(log_prob * ret).mean()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                logging_losses.append(loss.item())\n",
    "        return {\"loss\": logging_losses}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the policy\n",
    "Note that `BasePolicy` itself inherits from `torch.nn.Module`. As a result, you can consider all Policy modules as a Torch Module. They share similar APIs.\n",
    "\n",
    "Firstly we will initialise a new REINFORCE policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.discrete import Actor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "state_shape = 4\n",
    "action_shape = 2\n",
    "net = Net(state_shape, hidden_sizes=[16, 16], device=\"cpu\")\n",
    "actor= Actor(net, action_shape, device=\"cpu\").to(\"cpu\")\n",
    "optim = torch.optim.Adam(actor.parameters(), lr=0.0003)\n",
    "\n",
    "policy = REINFORCEPolicy(actor, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REINFORCEPolicy(\n",
      "  (actor): Actor(\n",
      "    (preprocess): Net(\n",
      "      (model): MLP(\n",
      "        (model): Sequential(\n",
      "          (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (3): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (last): MLP(\n",
      "      (model): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 16])\n",
      "torch.Size([16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(policy)\n",
    "print(\"==================================\")\n",
    "for para in policy.parameters():\n",
    "    print(para.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making decision\n",
    "Given a batch of observations, the policy can return a batch of actions and other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(\n",
      "    act: tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "                 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "                 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "                 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
      "                 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "                 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "                 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "                 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "                 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "                 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0]),\n",
      "    dist: Categorical(probs: torch.Size([256, 2])),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "obs_batch = Batch(obs=np.ones(shape=(256, 4)))\n",
    "action = policy(obs_batch)  # forward() method is called\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load models\n",
    "Naturally, Tianshou Policy can be saved and loaded like a normal Torch network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'policy.pth')\n",
    "assert policy.load_state_dict(torch.load('policy.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Updating\n",
    "We have to collect some data and save them in the ReplayBuffer updating our agent(policy). Typically we use collector to collect data, but we leave this part till later when we have learned the Collector in Tianshou. For now we generate some fake data.\n",
    "\n",
    "#### Generating fake data\n",
    "Firstly, we need to \"pretend\" that we are using the \"Policy\" to collect data. We plan to collect 10 data so that we can update our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplayBuffer()\n",
      "maxsize: 12, data length: 0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from tianshou.data import Batch, ReplayBuffer\n",
    "# a buffer is initialised with its maxsize set to 12\n",
    "buf = ReplayBuffer(size=12)\n",
    "print(buf)\n",
    "print(\"maxsize: {}, data length: {}\".format(buf.maxsize, len(buf)))\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are pretending to collect the first episode. The first episode ends at step 3 (perhaps we are performing too badly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()[0]\n",
    "for i in range(3):\n",
    "    act = policy(Batch(obs=obs[np.newaxis, :])).act.item()\n",
    "    obs_next, rew, terminated, truncated, info = env.step(act)\n",
    "    # pretend ending at step 3\n",
    "    terminated = True if i==2 else False\n",
    "    info[\"id\"] = i\n",
    "    buf.add(Batch(obs=obs, act=act, rew=rew, terminated=terminated, truncated=truncated, obs_next=obs_next, info=info))\n",
    "    obs = obs_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are pretending to collect the second episode. At step 7 the second episode still doesn't end, but we are unwilling to wait, so we stop collecting to update the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()[0]\n",
    "for i in range(3, 10):\n",
    "    act = policy(Batch(obs=obs[np.newaxis, :])).act.item()\n",
    "    obs_next, rew, terminated, truncated, info = env.step(act)\n",
    "    # pretend this episode never end\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    info[\"id\"] = i\n",
    "    buf.add(Batch(obs=obs, act=act, rew=rew, terminated=terminated, truncated=truncated, obs_next=obs_next, info=info))\n",
    "    obs = obs_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our replay buffer looks like this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplayBuffer(\n",
      "    obs: array([[ 0.00588752, -0.01020907, -0.03662223, -0.00836865],\n",
      "                [ 0.00568334,  0.18541844, -0.0367896 , -0.3123777 ],\n",
      "                [ 0.00939171, -0.00916062, -0.04303716, -0.0315203 ],\n",
      "                [ 0.04263549,  0.00945875,  0.01780597,  0.01071217],\n",
      "                [ 0.04282467, -0.18591398,  0.01802021,  0.30895948],\n",
      "                [ 0.03910639,  0.00894665,  0.0241994 ,  0.02201366],\n",
      "                [ 0.03928532,  0.20371334,  0.02463967, -0.26293692],\n",
      "                [ 0.04335959,  0.00824851,  0.01938093,  0.03741467],\n",
      "                [ 0.04352456,  0.20308726,  0.02012923, -0.24909092],\n",
      "                [ 0.0475863 ,  0.39791605,  0.01514741, -0.5353573 ],\n",
      "                [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "                [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
      "               dtype=float32),\n",
      "    info: Batch(\n",
      "              id: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 0]),\n",
      "          ),\n",
      "    rew: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.]),\n",
      "    terminated: array([False, False,  True, False, False, False, False, False, False,\n",
      "                       False, False, False]),\n",
      "    act: array([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0]),\n",
      "    obs_next: array([[ 0.00568334,  0.18541844, -0.0367896 , -0.3123777 ],\n",
      "                     [ 0.00939171, -0.00916062, -0.04303716, -0.0315203 ],\n",
      "                     [ 0.0092085 , -0.20363982, -0.04366757,  0.24727936],\n",
      "                     [ 0.04282467, -0.18591398,  0.01802021,  0.30895948],\n",
      "                     [ 0.03910639,  0.00894665,  0.0241994 ,  0.02201366],\n",
      "                     [ 0.03928532,  0.20371334,  0.02463967, -0.26293692],\n",
      "                     [ 0.04335959,  0.00824851,  0.01938093,  0.03741467],\n",
      "                     [ 0.04352456,  0.20308726,  0.02012923, -0.24909092],\n",
      "                     [ 0.0475863 ,  0.39791605,  0.01514741, -0.5353573 ],\n",
      "                     [ 0.05554462,  0.2025844 ,  0.00444026, -0.2379403 ],\n",
      "                     [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "                     [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
      "                    dtype=float32),\n",
      "    truncated: array([False, False, False, False, False, False, False, False, False,\n",
      "                      False, False, False]),\n",
      "    done: array([False, False,  True, False, False, False, False, False, False,\n",
      "                 False, False, False]),\n",
      ")\n",
      "maxsize: 12, data length: 10\n"
     ]
    }
   ],
   "source": [
    "print(buf)\n",
    "print(\"maxsize: {}, data length: {}\".format(buf.maxsize, len(buf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updates\n",
    "Now we have got a replay buffer with 10 data steps in it. We can call `Policy.update()` to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.2938356399536133,\n",
       "  2.2932286262512207,\n",
       "  2.2926220893859863,\n",
       "  2.292015552520752,\n",
       "  2.2914087772369385,\n",
       "  2.290802001953125]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 means sample all data from the buffer\n",
    "# batch_size=10 defines the training batch size\n",
    "# repeat=6 means repeat the training for 6 times\n",
    "policy.update(0, buf, batch_size=10, repeat=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
